---
title: "Assessing Exercise Quality"
author: "Gabriel Martins Dias"
date: "17/03/2015"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)

library(caret)
```

The goal of this report is to predict the manner in which people did their 
exercises based on the data from wearable sensors.

## Pre-processing

In order to make the results reproducible and consistent with future executions, 
we set a seed value.

```{r}
set.seed(1234)
```

## Fetching the training data

The data used for training was generated by accelerometers on the belt, forearm, 
arm, and dumbell of 6 participants. 
They were asked to perform barbell lifts correctly and incorrectly in 5 
different ways, which were classified into different classes of movements.

```{r}
#fetch training data
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
              destfile = "pml-training.csv", 
              method = "curl")
data <- read.csv("pml-training.csv")
```

## Cleaning the data

First, let us observe a summary of the data:

```{r}
summary(data)
```

First, we can remove the variables "X", "user_name", "raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp", "new_window" and "num_window", because 
they are only used to indentify the users and their tries, and are independent 
from the experiments.

```{r}
subset.data <- data[, -c(1,2,3,4,5,6,7)]
```

As we can observe, most of the features have a high number of NA's.
Therefore, we can start the process of cleaning the data by removing the 
variables with less than 90% of non-NA rows from the predictions.


```{r}
na.values <- is.na(subset.data)
na.rates <- colSums(na.values) / nrow(subset.data)

subset.data <- subset.data[ ,(na.rates < 0.1)]
```

The third step is to remove the columns after observing that they contain
non-numeric data, because they also represent absence of data.

```{r}
non.numeric.values <- mapply(is.numeric, subset.data, SIMPLIFY = TRUE)
non.numeric.values["classe"] <- TRUE # avoid removing the outcome
subset.data <- subset.data[ , non.numeric.values]
```

## Creating a model

We include the option to use a cross validation during the training phase of the Random Forest algorithm.
It is a K-fold cross validation with K = $10$.

```{r}
trainIndex <- createDataPartition(subset.data$classe, p = 0.6, list = FALSE)
training <- subset.data[trainIndex, ]
testing <- subset.data[-trainIndex, ]

fit <- train(classe ~ ., 
             data = training, 
             method="rf",
             prox=TRUE,
             allowParallel=TRUE, 
             trControl=trainControl(method = "cv", number = 10 ))

print(fit)
```

From the summary above, we can observe that the accuracy of the predictions was 
around $99.0\%$ during the training phase.

## Out of sample testing

As we split the data above into two sets (training / testing), we did not use
the test set to set up the model.
Now we can compare the outcomes from the predictions using the model created

```{r}
outcomes <- predict(fit, newdata = testing)
rate <- sum(testing$classe == outcomes) / length(outcomes)
print(rate)
```

Therefore, the expected out of sample accuracy is $99.17\%$ (i.e., the expected error 
rate is $0.83\%$). 

The following plot show the outcome of the predictions groupped by the real observations.

```{r}
# join the predictions and the real observations
# select only the wrong predictions (outcome != classe)
subset.wrong.predictions <- subset(cbind(testing, data.frame(outcome = outcomes)), outcome != classe)
ggplot(data = subset.wrong.predictions, aes(x = outcome, fill = classe)) +
        geom_histogram() + 
        ggtitle("Number of wrong predictions")
```

We can observe that there were more errors when the class was predicted to be "C". 

## Submission

Let us run the prediction model created above through the real data for the submission.

```{r}
# fetch testing data
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
              destfile = "pml-testing.csv", 
              method = "curl")
testing.real <- read.csv("pml-testing.csv")

outcomes <- predict(fit, newdata = testing.real)
print(outcomes)

```

```{r, echo = FALSE}
#saving the final anwsers for the submission
pml_write_files = function(outcomes){
  for(i in seq(1, along.with = outcomes)){
    filename = paste0("problem_id_",i,".txt")
    write.table(outcomes[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(outcomes)
```